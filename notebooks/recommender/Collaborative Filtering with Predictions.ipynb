{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_Id</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Semarang, Jawa Tengah</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bekasi, Jawa Barat</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cirebon, Jawa Barat</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bekasi, Jawa Barat</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Lampung, Sumatera Selatan</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>296</td>\n",
       "      <td>Lampung, Sumatera Selatan</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>297</td>\n",
       "      <td>Palembang, Sumatera Selatan</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>Bogor, Jawa Barat</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>299</td>\n",
       "      <td>Sragen, Jawa Tengah</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>Ponorogo, Jawa Timur</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_Id                     Location  Age\n",
       "0          1        Semarang, Jawa Tengah   20\n",
       "1          2           Bekasi, Jawa Barat   21\n",
       "2          3          Cirebon, Jawa Barat   23\n",
       "3          4           Bekasi, Jawa Barat   21\n",
       "4          5    Lampung, Sumatera Selatan   20\n",
       "..       ...                          ...  ...\n",
       "295      296    Lampung, Sumatera Selatan   31\n",
       "296      297  Palembang, Sumatera Selatan   39\n",
       "297      298            Bogor, Jawa Barat   38\n",
       "298      299          Sragen, Jawa Tengah   27\n",
       "299      300         Ponorogo, Jawa Timur   26\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.read_csv(\"../data/datasets/user.csv\")\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_Id</th>\n",
       "      <th>Place_Id</th>\n",
       "      <th>Place_Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>300</td>\n",
       "      <td>425</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>300</td>\n",
       "      <td>311</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>300</td>\n",
       "      <td>279</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>300</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User_Id  Place_Id  Place_Ratings\n",
       "0           1       179              3\n",
       "1           1       344              2\n",
       "2           1         5              5\n",
       "3           1       373              3\n",
       "4           1       101              4\n",
       "...       ...       ...            ...\n",
       "9995      300       425              2\n",
       "9996      300        64              4\n",
       "9997      300       311              3\n",
       "9998      300       279              4\n",
       "9999      300       163              2\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat = pd.read_csv(\"../data/datasets/tourism_rating.csv\")\n",
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Place_Id</th>\n",
       "      <th>Place_Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Time_Minutes</th>\n",
       "      <th>Coordinate</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>new_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Monumen Nasional</td>\n",
       "      <td>Monumen Nasional atau yang populer disingkat d...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>20000</td>\n",
       "      <td>4.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>{'lat': -6.1753924, 'lng': 106.8271528}</td>\n",
       "      <td>-6.175392</td>\n",
       "      <td>106.827153</td>\n",
       "      <td>['Budaya', 'Keluarga']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Kota Tua</td>\n",
       "      <td>Kota tua di Jakarta, yang juga bernama Kota Tu...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>90.0</td>\n",
       "      <td>{'lat': -6.137644799999999, 'lng': 106.8171245}</td>\n",
       "      <td>-6.137645</td>\n",
       "      <td>106.817125</td>\n",
       "      <td>['Budaya']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dunia Fantasi</td>\n",
       "      <td>Dunia Fantasi atau disebut juga Dufan adalah t...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>270000</td>\n",
       "      <td>4.6</td>\n",
       "      <td>360.0</td>\n",
       "      <td>{'lat': -6.125312399999999, 'lng': 106.8335377}</td>\n",
       "      <td>-6.125312</td>\n",
       "      <td>106.833538</td>\n",
       "      <td>['Hiburan', 'Keluarga']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Taman Mini Indonesia Indah (TMII)</td>\n",
       "      <td>Taman Mini Indonesia Indah merupakan suatu kaw...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>10000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'lat': -6.302445899999999, 'lng': 106.8951559}</td>\n",
       "      <td>-6.302446</td>\n",
       "      <td>106.895156</td>\n",
       "      <td>['Budaya', 'Hiburan']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Atlantis Water Adventure</td>\n",
       "      <td>Atlantis Water Adventure atau dikenal dengan A...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>94000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>{'lat': -6.12419, 'lng': 106.839134}</td>\n",
       "      <td>-6.124190</td>\n",
       "      <td>106.839134</td>\n",
       "      <td>['Petualangan', 'Hiburan']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>433</td>\n",
       "      <td>Museum Mpu Tantular</td>\n",
       "      <td>Museum Negeri Mpu Tantular adalah sebuah museu...</td>\n",
       "      <td>Surabaya</td>\n",
       "      <td>2000</td>\n",
       "      <td>4.4</td>\n",
       "      <td>45.0</td>\n",
       "      <td>{'lat': -7.4338593, 'lng': 112.7199058}</td>\n",
       "      <td>-7.433859</td>\n",
       "      <td>112.719906</td>\n",
       "      <td>['Sejarah', 'Budaya']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>434</td>\n",
       "      <td>Taman Bungkul</td>\n",
       "      <td>Taman Bungkul adalah taman wisata kota yang te...</td>\n",
       "      <td>Surabaya</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'lat': -7.291346799999999, 'lng': 112.7398218}</td>\n",
       "      <td>-7.291347</td>\n",
       "      <td>112.739822</td>\n",
       "      <td>['Alam', 'Relaksasi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>435</td>\n",
       "      <td>Taman Air Mancur Menari Kenjeran</td>\n",
       "      <td>Air mancur menari atau dancing fountain juga a...</td>\n",
       "      <td>Surabaya</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>45.0</td>\n",
       "      <td>{'lat': -7.2752955, 'lng': 112.7549381}</td>\n",
       "      <td>-7.275296</td>\n",
       "      <td>112.754938</td>\n",
       "      <td>['Alam', 'Hiburan']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>436</td>\n",
       "      <td>Taman Flora Bratang Surabaya</td>\n",
       "      <td>Taman Flora adalah salah satu taman kota di Su...</td>\n",
       "      <td>Surabaya</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'lat': -7.294330299999999, 'lng': 112.7617534}</td>\n",
       "      <td>-7.294330</td>\n",
       "      <td>112.761753</td>\n",
       "      <td>['Alam', 'Relaksasi']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>437</td>\n",
       "      <td>Gereja Perawan Maria Tak Berdosa Surabaya</td>\n",
       "      <td>Gereja Katolik Kelahiran Santa Perawan Maria m...</td>\n",
       "      <td>Surabaya</td>\n",
       "      <td>10000</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'lat': -7.2420758, 'lng': 112.7368158}</td>\n",
       "      <td>-7.242076</td>\n",
       "      <td>112.736816</td>\n",
       "      <td>['Budaya']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Place_Id                                 Place_Name  \\\n",
       "0           1                           Monumen Nasional   \n",
       "1           2                                   Kota Tua   \n",
       "2           3                              Dunia Fantasi   \n",
       "3           4          Taman Mini Indonesia Indah (TMII)   \n",
       "4           5                   Atlantis Water Adventure   \n",
       "..        ...                                        ...   \n",
       "432       433                        Museum Mpu Tantular   \n",
       "433       434                              Taman Bungkul   \n",
       "434       435           Taman Air Mancur Menari Kenjeran   \n",
       "435       436               Taman Flora Bratang Surabaya   \n",
       "436       437  Gereja Perawan Maria Tak Berdosa Surabaya   \n",
       "\n",
       "                                           Description      City   Price  \\\n",
       "0    Monumen Nasional atau yang populer disingkat d...   Jakarta   20000   \n",
       "1    Kota tua di Jakarta, yang juga bernama Kota Tu...   Jakarta       0   \n",
       "2    Dunia Fantasi atau disebut juga Dufan adalah t...   Jakarta  270000   \n",
       "3    Taman Mini Indonesia Indah merupakan suatu kaw...   Jakarta   10000   \n",
       "4    Atlantis Water Adventure atau dikenal dengan A...   Jakarta   94000   \n",
       "..                                                 ...       ...     ...   \n",
       "432  Museum Negeri Mpu Tantular adalah sebuah museu...  Surabaya    2000   \n",
       "433  Taman Bungkul adalah taman wisata kota yang te...  Surabaya       0   \n",
       "434  Air mancur menari atau dancing fountain juga a...  Surabaya       0   \n",
       "435  Taman Flora adalah salah satu taman kota di Su...  Surabaya       0   \n",
       "436  Gereja Katolik Kelahiran Santa Perawan Maria m...  Surabaya   10000   \n",
       "\n",
       "     Rating  Time_Minutes                                       Coordinate  \\\n",
       "0       4.6          15.0          {'lat': -6.1753924, 'lng': 106.8271528}   \n",
       "1       4.6          90.0  {'lat': -6.137644799999999, 'lng': 106.8171245}   \n",
       "2       4.6         360.0  {'lat': -6.125312399999999, 'lng': 106.8335377}   \n",
       "3       4.5           NaN  {'lat': -6.302445899999999, 'lng': 106.8951559}   \n",
       "4       4.5          60.0             {'lat': -6.12419, 'lng': 106.839134}   \n",
       "..      ...           ...                                              ...   \n",
       "432     4.4          45.0          {'lat': -7.4338593, 'lng': 112.7199058}   \n",
       "433     4.6           NaN  {'lat': -7.291346799999999, 'lng': 112.7398218}   \n",
       "434     4.4          45.0          {'lat': -7.2752955, 'lng': 112.7549381}   \n",
       "435     4.6           NaN  {'lat': -7.294330299999999, 'lng': 112.7617534}   \n",
       "436     4.8           NaN          {'lat': -7.2420758, 'lng': 112.7368158}   \n",
       "\n",
       "          Lat        Long                new_category  \n",
       "0   -6.175392  106.827153      ['Budaya', 'Keluarga']  \n",
       "1   -6.137645  106.817125                  ['Budaya']  \n",
       "2   -6.125312  106.833538     ['Hiburan', 'Keluarga']  \n",
       "3   -6.302446  106.895156       ['Budaya', 'Hiburan']  \n",
       "4   -6.124190  106.839134  ['Petualangan', 'Hiburan']  \n",
       "..        ...         ...                         ...  \n",
       "432 -7.433859  112.719906       ['Sejarah', 'Budaya']  \n",
       "433 -7.291347  112.739822       ['Alam', 'Relaksasi']  \n",
       "434 -7.275296  112.754938         ['Alam', 'Hiburan']  \n",
       "435 -7.294330  112.761753       ['Alam', 'Relaksasi']  \n",
       "436 -7.242076  112.736816                  ['Budaya']  \n",
       "\n",
       "[437 rows x 11 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update = pd.read_csv('../data/datasets/updated/tourism_with_id_updated.csv')\n",
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list User_Id :  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300]\n",
      "encoded User_Id :  {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59, 61: 60, 62: 61, 63: 62, 64: 63, 65: 64, 66: 65, 67: 66, 68: 67, 69: 68, 70: 69, 71: 70, 72: 71, 73: 72, 74: 73, 75: 74, 76: 75, 77: 76, 78: 77, 79: 78, 80: 79, 81: 80, 82: 81, 83: 82, 84: 83, 85: 84, 86: 85, 87: 86, 88: 87, 89: 88, 90: 89, 91: 90, 92: 91, 93: 92, 94: 93, 95: 94, 96: 95, 97: 96, 98: 97, 99: 98, 100: 99, 101: 100, 102: 101, 103: 102, 104: 103, 105: 104, 106: 105, 107: 106, 108: 107, 109: 108, 110: 109, 111: 110, 112: 111, 113: 112, 114: 113, 115: 114, 116: 115, 117: 116, 118: 117, 119: 118, 120: 119, 121: 120, 122: 121, 123: 122, 124: 123, 125: 124, 126: 125, 127: 126, 128: 127, 129: 128, 130: 129, 131: 130, 132: 131, 133: 132, 134: 133, 135: 134, 136: 135, 137: 136, 138: 137, 139: 138, 140: 139, 141: 140, 142: 141, 143: 142, 144: 143, 145: 144, 146: 145, 147: 146, 148: 147, 149: 148, 150: 149, 151: 150, 152: 151, 153: 152, 154: 153, 155: 154, 156: 155, 157: 156, 158: 157, 159: 158, 160: 159, 161: 160, 162: 161, 163: 162, 164: 163, 165: 164, 166: 165, 167: 166, 168: 167, 169: 168, 170: 169, 171: 170, 172: 171, 173: 172, 174: 173, 175: 174, 176: 175, 177: 176, 178: 177, 179: 178, 180: 179, 181: 180, 182: 181, 183: 182, 184: 183, 185: 184, 186: 185, 187: 186, 188: 187, 189: 188, 190: 189, 191: 190, 192: 191, 193: 192, 194: 193, 195: 194, 196: 195, 197: 196, 198: 197, 199: 198, 200: 199, 201: 200, 202: 201, 203: 202, 204: 203, 205: 204, 206: 205, 207: 206, 208: 207, 209: 208, 210: 209, 211: 210, 212: 211, 213: 212, 214: 213, 215: 214, 216: 215, 217: 216, 218: 217, 219: 218, 220: 219, 221: 220, 222: 221, 223: 222, 224: 223, 225: 224, 226: 225, 227: 226, 228: 227, 229: 228, 230: 229, 231: 230, 232: 231, 233: 232, 234: 233, 235: 234, 236: 235, 237: 236, 238: 237, 239: 238, 240: 239, 241: 240, 242: 241, 243: 242, 244: 243, 245: 244, 246: 245, 247: 246, 248: 247, 249: 248, 250: 249, 251: 250, 252: 251, 253: 252, 254: 253, 255: 254, 256: 255, 257: 256, 258: 257, 259: 258, 260: 259, 261: 260, 262: 261, 263: 262, 264: 263, 265: 264, 266: 265, 267: 266, 268: 267, 269: 268, 270: 269, 271: 270, 272: 271, 273: 272, 274: 273, 275: 274, 276: 275, 277: 276, 278: 277, 279: 278, 280: 279, 281: 280, 282: 281, 283: 282, 284: 283, 285: 284, 286: 285, 287: 286, 288: 287, 289: 288, 290: 289, 291: 290, 292: 291, 293: 292, 294: 293, 295: 294, 296: 295, 297: 296, 298: 297, 299: 298, 300: 299}\n",
      "decode angka ke User_Id :  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60, 60: 61, 61: 62, 62: 63, 63: 64, 64: 65, 65: 66, 66: 67, 67: 68, 68: 69, 69: 70, 70: 71, 71: 72, 72: 73, 73: 74, 74: 75, 75: 76, 76: 77, 77: 78, 78: 79, 79: 80, 80: 81, 81: 82, 82: 83, 83: 84, 84: 85, 85: 86, 86: 87, 87: 88, 88: 89, 89: 90, 90: 91, 91: 92, 92: 93, 93: 94, 94: 95, 95: 96, 96: 97, 97: 98, 98: 99, 99: 100, 100: 101, 101: 102, 102: 103, 103: 104, 104: 105, 105: 106, 106: 107, 107: 108, 108: 109, 109: 110, 110: 111, 111: 112, 112: 113, 113: 114, 114: 115, 115: 116, 116: 117, 117: 118, 118: 119, 119: 120, 120: 121, 121: 122, 122: 123, 123: 124, 124: 125, 125: 126, 126: 127, 127: 128, 128: 129, 129: 130, 130: 131, 131: 132, 132: 133, 133: 134, 134: 135, 135: 136, 136: 137, 137: 138, 138: 139, 139: 140, 140: 141, 141: 142, 142: 143, 143: 144, 144: 145, 145: 146, 146: 147, 147: 148, 148: 149, 149: 150, 150: 151, 151: 152, 152: 153, 153: 154, 154: 155, 155: 156, 156: 157, 157: 158, 158: 159, 159: 160, 160: 161, 161: 162, 162: 163, 163: 164, 164: 165, 165: 166, 166: 167, 167: 168, 168: 169, 169: 170, 170: 171, 171: 172, 172: 173, 173: 174, 174: 175, 175: 176, 176: 177, 177: 178, 178: 179, 179: 180, 180: 181, 181: 182, 182: 183, 183: 184, 184: 185, 185: 186, 186: 187, 187: 188, 188: 189, 189: 190, 190: 191, 191: 192, 192: 193, 193: 194, 194: 195, 195: 196, 196: 197, 197: 198, 198: 199, 199: 200, 200: 201, 201: 202, 202: 203, 203: 204, 204: 205, 205: 206, 206: 207, 207: 208, 208: 209, 209: 210, 210: 211, 211: 212, 212: 213, 213: 214, 214: 215, 215: 216, 216: 217, 217: 218, 218: 219, 219: 220, 220: 221, 221: 222, 222: 223, 223: 224, 224: 225, 225: 226, 226: 227, 227: 228, 228: 229, 229: 230, 230: 231, 231: 232, 232: 233, 233: 234, 234: 235, 235: 236, 236: 237, 237: 238, 238: 239, 239: 240, 240: 241, 241: 242, 242: 243, 243: 244, 244: 245, 245: 246, 246: 247, 247: 248, 248: 249, 249: 250, 250: 251, 251: 252, 252: 253, 253: 254, 254: 255, 255: 256, 256: 257, 257: 258, 258: 259, 259: 260, 260: 261, 261: 262, 262: 263, 263: 264, 264: 265, 265: 266, 266: 267, 267: 268, 268: 269, 269: 270, 270: 271, 271: 272, 272: 273, 273: 274, 274: 275, 275: 276, 276: 277, 277: 278, 278: 279, 279: 280, 280: 281, 281: 282, 282: 283, 283: 284, 284: 285, 285: 286, 286: 287, 287: 288, 288: 289, 289: 290, 290: 291, 291: 292, 292: 293, 293: 294, 294: 295, 295: 296, 296: 297, 297: 298, 298: 299, 299: 300}\n"
     ]
    }
   ],
   "source": [
    "user_ids = rat['User_Id'].unique().tolist()\n",
    "print('list User_Id : ', user_ids)\n",
    "\n",
    "user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "print('encoded User_Id : ', user_to_user_encoded)\n",
    "\n",
    "user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}\n",
    "print('decode angka ke User_Id : ', user_encoded_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list Place_Id :  [179, 344, 5, 373, 101, 312, 258, 20, 154, 393, 103, 208, 89, 405, 41, 336, 67, 292, 222, 76, 428, 15, 246, 265, 21, 328, 307, 302, 48, 147, 2, 322, 23, 85, 371, 78, 111, 107, 185, 413, 389, 437, 105, 176, 407, 281, 167, 384, 426, 390, 367, 166, 18, 321, 262, 223, 17, 319, 310, 421, 202, 283, 250, 118, 54, 70, 33, 227, 86, 249, 131, 382, 193, 104, 24, 128, 134, 228, 237, 100, 198, 50, 376, 268, 242, 82, 348, 409, 88, 4, 377, 43, 91, 44, 14, 406, 383, 229, 290, 294, 309, 74, 97, 138, 436, 395, 300, 68, 55, 434, 9, 151, 61, 159, 397, 391, 301, 143, 27, 190, 346, 381, 368, 433, 420, 335, 115, 334, 219, 178, 266, 92, 64, 102, 257, 243, 116, 359, 121, 385, 398, 119, 298, 410, 224, 379, 255, 269, 213, 126, 192, 183, 129, 303, 296, 99, 148, 13, 247, 31, 77, 158, 306, 98, 235, 45, 173, 275, 25, 130, 164, 267, 56, 30, 238, 402, 196, 417, 304, 206, 211, 28, 264, 403, 253, 331, 188, 416, 378, 233, 95, 204, 225, 341, 145, 36, 168, 12, 280, 236, 42, 90, 75, 63, 205, 234, 284, 230, 197, 339, 35, 226, 430, 37, 109, 355, 186, 149, 6, 150, 11, 127, 429, 194, 49, 369, 94, 274, 72, 46, 65, 203, 139, 141, 71, 52, 293, 422, 214, 114, 191, 156, 177, 163, 142, 245, 325, 259, 396, 96, 8, 362, 435, 59, 171, 277, 342, 365, 340, 260, 187, 297, 207, 248, 153, 239, 360, 170, 357, 73, 201, 252, 414, 132, 26, 347, 270, 16, 19, 318, 256, 181, 199, 288, 330, 3, 47, 32, 308, 210, 279, 289, 332, 80, 375, 182, 412, 272, 184, 113, 399, 261, 112, 327, 418, 84, 400, 195, 34, 295, 38, 165, 212, 124, 432, 317, 333, 81, 374, 136, 278, 108, 404, 83, 425, 22, 349, 411, 241, 427, 209, 323, 137, 315, 62, 244, 135, 169, 366, 152, 29, 60, 240, 200, 161, 66, 372, 338, 311, 53, 392, 299, 386, 337, 326, 423, 352, 172, 286, 251, 162, 356, 93, 160, 125, 316, 123, 174, 221, 408, 263, 358, 353, 401, 364, 215, 110, 144, 40, 291, 431, 120, 216, 313, 157, 282, 231, 324, 329, 345, 39, 273, 122, 79, 388, 380, 343, 254, 117, 133, 1, 354, 287, 155, 285, 58, 69, 305, 218, 220, 189, 106, 87, 351, 363, 175, 180, 415, 387, 394, 424, 146, 232, 314, 320, 276, 217, 57, 51, 361, 419, 271, 370, 350, 10, 7, 140]\n",
      "encoded Place_Id :  {179: 0, 344: 1, 5: 2, 373: 3, 101: 4, 312: 5, 258: 6, 20: 7, 154: 8, 393: 9, 103: 10, 208: 11, 89: 12, 405: 13, 41: 14, 336: 15, 67: 16, 292: 17, 222: 18, 76: 19, 428: 20, 15: 21, 246: 22, 265: 23, 21: 24, 328: 25, 307: 26, 302: 27, 48: 28, 147: 29, 2: 30, 322: 31, 23: 32, 85: 33, 371: 34, 78: 35, 111: 36, 107: 37, 185: 38, 413: 39, 389: 40, 437: 41, 105: 42, 176: 43, 407: 44, 281: 45, 167: 46, 384: 47, 426: 48, 390: 49, 367: 50, 166: 51, 18: 52, 321: 53, 262: 54, 223: 55, 17: 56, 319: 57, 310: 58, 421: 59, 202: 60, 283: 61, 250: 62, 118: 63, 54: 64, 70: 65, 33: 66, 227: 67, 86: 68, 249: 69, 131: 70, 382: 71, 193: 72, 104: 73, 24: 74, 128: 75, 134: 76, 228: 77, 237: 78, 100: 79, 198: 80, 50: 81, 376: 82, 268: 83, 242: 84, 82: 85, 348: 86, 409: 87, 88: 88, 4: 89, 377: 90, 43: 91, 91: 92, 44: 93, 14: 94, 406: 95, 383: 96, 229: 97, 290: 98, 294: 99, 309: 100, 74: 101, 97: 102, 138: 103, 436: 104, 395: 105, 300: 106, 68: 107, 55: 108, 434: 109, 9: 110, 151: 111, 61: 112, 159: 113, 397: 114, 391: 115, 301: 116, 143: 117, 27: 118, 190: 119, 346: 120, 381: 121, 368: 122, 433: 123, 420: 124, 335: 125, 115: 126, 334: 127, 219: 128, 178: 129, 266: 130, 92: 131, 64: 132, 102: 133, 257: 134, 243: 135, 116: 136, 359: 137, 121: 138, 385: 139, 398: 140, 119: 141, 298: 142, 410: 143, 224: 144, 379: 145, 255: 146, 269: 147, 213: 148, 126: 149, 192: 150, 183: 151, 129: 152, 303: 153, 296: 154, 99: 155, 148: 156, 13: 157, 247: 158, 31: 159, 77: 160, 158: 161, 306: 162, 98: 163, 235: 164, 45: 165, 173: 166, 275: 167, 25: 168, 130: 169, 164: 170, 267: 171, 56: 172, 30: 173, 238: 174, 402: 175, 196: 176, 417: 177, 304: 178, 206: 179, 211: 180, 28: 181, 264: 182, 403: 183, 253: 184, 331: 185, 188: 186, 416: 187, 378: 188, 233: 189, 95: 190, 204: 191, 225: 192, 341: 193, 145: 194, 36: 195, 168: 196, 12: 197, 280: 198, 236: 199, 42: 200, 90: 201, 75: 202, 63: 203, 205: 204, 234: 205, 284: 206, 230: 207, 197: 208, 339: 209, 35: 210, 226: 211, 430: 212, 37: 213, 109: 214, 355: 215, 186: 216, 149: 217, 6: 218, 150: 219, 11: 220, 127: 221, 429: 222, 194: 223, 49: 224, 369: 225, 94: 226, 274: 227, 72: 228, 46: 229, 65: 230, 203: 231, 139: 232, 141: 233, 71: 234, 52: 235, 293: 236, 422: 237, 214: 238, 114: 239, 191: 240, 156: 241, 177: 242, 163: 243, 142: 244, 245: 245, 325: 246, 259: 247, 396: 248, 96: 249, 8: 250, 362: 251, 435: 252, 59: 253, 171: 254, 277: 255, 342: 256, 365: 257, 340: 258, 260: 259, 187: 260, 297: 261, 207: 262, 248: 263, 153: 264, 239: 265, 360: 266, 170: 267, 357: 268, 73: 269, 201: 270, 252: 271, 414: 272, 132: 273, 26: 274, 347: 275, 270: 276, 16: 277, 19: 278, 318: 279, 256: 280, 181: 281, 199: 282, 288: 283, 330: 284, 3: 285, 47: 286, 32: 287, 308: 288, 210: 289, 279: 290, 289: 291, 332: 292, 80: 293, 375: 294, 182: 295, 412: 296, 272: 297, 184: 298, 113: 299, 399: 300, 261: 301, 112: 302, 327: 303, 418: 304, 84: 305, 400: 306, 195: 307, 34: 308, 295: 309, 38: 310, 165: 311, 212: 312, 124: 313, 432: 314, 317: 315, 333: 316, 81: 317, 374: 318, 136: 319, 278: 320, 108: 321, 404: 322, 83: 323, 425: 324, 22: 325, 349: 326, 411: 327, 241: 328, 427: 329, 209: 330, 323: 331, 137: 332, 315: 333, 62: 334, 244: 335, 135: 336, 169: 337, 366: 338, 152: 339, 29: 340, 60: 341, 240: 342, 200: 343, 161: 344, 66: 345, 372: 346, 338: 347, 311: 348, 53: 349, 392: 350, 299: 351, 386: 352, 337: 353, 326: 354, 423: 355, 352: 356, 172: 357, 286: 358, 251: 359, 162: 360, 356: 361, 93: 362, 160: 363, 125: 364, 316: 365, 123: 366, 174: 367, 221: 368, 408: 369, 263: 370, 358: 371, 353: 372, 401: 373, 364: 374, 215: 375, 110: 376, 144: 377, 40: 378, 291: 379, 431: 380, 120: 381, 216: 382, 313: 383, 157: 384, 282: 385, 231: 386, 324: 387, 329: 388, 345: 389, 39: 390, 273: 391, 122: 392, 79: 393, 388: 394, 380: 395, 343: 396, 254: 397, 117: 398, 133: 399, 1: 400, 354: 401, 287: 402, 155: 403, 285: 404, 58: 405, 69: 406, 305: 407, 218: 408, 220: 409, 189: 410, 106: 411, 87: 412, 351: 413, 363: 414, 175: 415, 180: 416, 415: 417, 387: 418, 394: 419, 424: 420, 146: 421, 232: 422, 314: 423, 320: 424, 276: 425, 217: 426, 57: 427, 51: 428, 361: 429, 419: 430, 271: 431, 370: 432, 350: 433, 10: 434, 7: 435, 140: 436}\n",
      "decode angka ke Place_Id :  {0: 179, 1: 344, 2: 5, 3: 373, 4: 101, 5: 312, 6: 258, 7: 20, 8: 154, 9: 393, 10: 103, 11: 208, 12: 89, 13: 405, 14: 41, 15: 336, 16: 67, 17: 292, 18: 222, 19: 76, 20: 428, 21: 15, 22: 246, 23: 265, 24: 21, 25: 328, 26: 307, 27: 302, 28: 48, 29: 147, 30: 2, 31: 322, 32: 23, 33: 85, 34: 371, 35: 78, 36: 111, 37: 107, 38: 185, 39: 413, 40: 389, 41: 437, 42: 105, 43: 176, 44: 407, 45: 281, 46: 167, 47: 384, 48: 426, 49: 390, 50: 367, 51: 166, 52: 18, 53: 321, 54: 262, 55: 223, 56: 17, 57: 319, 58: 310, 59: 421, 60: 202, 61: 283, 62: 250, 63: 118, 64: 54, 65: 70, 66: 33, 67: 227, 68: 86, 69: 249, 70: 131, 71: 382, 72: 193, 73: 104, 74: 24, 75: 128, 76: 134, 77: 228, 78: 237, 79: 100, 80: 198, 81: 50, 82: 376, 83: 268, 84: 242, 85: 82, 86: 348, 87: 409, 88: 88, 89: 4, 90: 377, 91: 43, 92: 91, 93: 44, 94: 14, 95: 406, 96: 383, 97: 229, 98: 290, 99: 294, 100: 309, 101: 74, 102: 97, 103: 138, 104: 436, 105: 395, 106: 300, 107: 68, 108: 55, 109: 434, 110: 9, 111: 151, 112: 61, 113: 159, 114: 397, 115: 391, 116: 301, 117: 143, 118: 27, 119: 190, 120: 346, 121: 381, 122: 368, 123: 433, 124: 420, 125: 335, 126: 115, 127: 334, 128: 219, 129: 178, 130: 266, 131: 92, 132: 64, 133: 102, 134: 257, 135: 243, 136: 116, 137: 359, 138: 121, 139: 385, 140: 398, 141: 119, 142: 298, 143: 410, 144: 224, 145: 379, 146: 255, 147: 269, 148: 213, 149: 126, 150: 192, 151: 183, 152: 129, 153: 303, 154: 296, 155: 99, 156: 148, 157: 13, 158: 247, 159: 31, 160: 77, 161: 158, 162: 306, 163: 98, 164: 235, 165: 45, 166: 173, 167: 275, 168: 25, 169: 130, 170: 164, 171: 267, 172: 56, 173: 30, 174: 238, 175: 402, 176: 196, 177: 417, 178: 304, 179: 206, 180: 211, 181: 28, 182: 264, 183: 403, 184: 253, 185: 331, 186: 188, 187: 416, 188: 378, 189: 233, 190: 95, 191: 204, 192: 225, 193: 341, 194: 145, 195: 36, 196: 168, 197: 12, 198: 280, 199: 236, 200: 42, 201: 90, 202: 75, 203: 63, 204: 205, 205: 234, 206: 284, 207: 230, 208: 197, 209: 339, 210: 35, 211: 226, 212: 430, 213: 37, 214: 109, 215: 355, 216: 186, 217: 149, 218: 6, 219: 150, 220: 11, 221: 127, 222: 429, 223: 194, 224: 49, 225: 369, 226: 94, 227: 274, 228: 72, 229: 46, 230: 65, 231: 203, 232: 139, 233: 141, 234: 71, 235: 52, 236: 293, 237: 422, 238: 214, 239: 114, 240: 191, 241: 156, 242: 177, 243: 163, 244: 142, 245: 245, 246: 325, 247: 259, 248: 396, 249: 96, 250: 8, 251: 362, 252: 435, 253: 59, 254: 171, 255: 277, 256: 342, 257: 365, 258: 340, 259: 260, 260: 187, 261: 297, 262: 207, 263: 248, 264: 153, 265: 239, 266: 360, 267: 170, 268: 357, 269: 73, 270: 201, 271: 252, 272: 414, 273: 132, 274: 26, 275: 347, 276: 270, 277: 16, 278: 19, 279: 318, 280: 256, 281: 181, 282: 199, 283: 288, 284: 330, 285: 3, 286: 47, 287: 32, 288: 308, 289: 210, 290: 279, 291: 289, 292: 332, 293: 80, 294: 375, 295: 182, 296: 412, 297: 272, 298: 184, 299: 113, 300: 399, 301: 261, 302: 112, 303: 327, 304: 418, 305: 84, 306: 400, 307: 195, 308: 34, 309: 295, 310: 38, 311: 165, 312: 212, 313: 124, 314: 432, 315: 317, 316: 333, 317: 81, 318: 374, 319: 136, 320: 278, 321: 108, 322: 404, 323: 83, 324: 425, 325: 22, 326: 349, 327: 411, 328: 241, 329: 427, 330: 209, 331: 323, 332: 137, 333: 315, 334: 62, 335: 244, 336: 135, 337: 169, 338: 366, 339: 152, 340: 29, 341: 60, 342: 240, 343: 200, 344: 161, 345: 66, 346: 372, 347: 338, 348: 311, 349: 53, 350: 392, 351: 299, 352: 386, 353: 337, 354: 326, 355: 423, 356: 352, 357: 172, 358: 286, 359: 251, 360: 162, 361: 356, 362: 93, 363: 160, 364: 125, 365: 316, 366: 123, 367: 174, 368: 221, 369: 408, 370: 263, 371: 358, 372: 353, 373: 401, 374: 364, 375: 215, 376: 110, 377: 144, 378: 40, 379: 291, 380: 431, 381: 120, 382: 216, 383: 313, 384: 157, 385: 282, 386: 231, 387: 324, 388: 329, 389: 345, 390: 39, 391: 273, 392: 122, 393: 79, 394: 388, 395: 380, 396: 343, 397: 254, 398: 117, 399: 133, 400: 1, 401: 354, 402: 287, 403: 155, 404: 285, 405: 58, 406: 69, 407: 305, 408: 218, 409: 220, 410: 189, 411: 106, 412: 87, 413: 351, 414: 363, 415: 175, 416: 180, 417: 415, 418: 387, 419: 394, 420: 424, 421: 146, 422: 232, 423: 314, 424: 320, 425: 276, 426: 217, 427: 57, 428: 51, 429: 361, 430: 419, 431: 271, 432: 370, 433: 350, 434: 10, 435: 7, 436: 140}\n"
     ]
    }
   ],
   "source": [
    "place_ids = rat['Place_Id'].unique().tolist()\n",
    "print('list Place_Id : ', place_ids)\n",
    "\n",
    "place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}\n",
    "print('encoded Place_Id : ', place_to_place_encoded)\n",
    "\n",
    "place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}\n",
    "print('decode angka ke Place_Id : ', place_encoded_to_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_Id</th>\n",
       "      <th>Place_Id</th>\n",
       "      <th>Place_Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>300</td>\n",
       "      <td>425</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>300</td>\n",
       "      <td>311</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>300</td>\n",
       "      <td>279</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>300</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User_Id  Place_Id  Place_Ratings\n",
       "0           1       179              3\n",
       "1           1       344              2\n",
       "2           1         5              5\n",
       "3           1       373              3\n",
       "4           1       101              4\n",
       "...       ...       ...            ...\n",
       "9995      300       425              2\n",
       "9996      300        64              4\n",
       "9997      300       311              3\n",
       "9998      300       279              4\n",
       "9999      300       163              2\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "collfil = rat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "collfil['user'] = collfil['User_Id'].map(user_to_user_encoded)\n",
    "collfil['place'] = collfil['Place_Id'].map(place_to_place_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users Count: 300\n",
      "Places Count: 437\n",
      "Min rating: 1.0\n",
      "Max rating: 5.0\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_to_user_encoded)\n",
    "num_places = len(place_encoded_to_place)\n",
    "\n",
    "collfil['rating'] = collfil['Place_Ratings'].values.astype(np.float32)\n",
    "\n",
    "min_rating = min(collfil['rating'])\n",
    "max_rating = max(collfil['rating'])\n",
    "\n",
    "print(f'Users Count: {num_users}')\n",
    "print(f'Places Count: {num_places}')\n",
    "print(f'Min rating: {min_rating}')\n",
    "print(f'Max rating: {max_rating}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_Id</th>\n",
       "      <th>Place_Id</th>\n",
       "      <th>Place_Ratings</th>\n",
       "      <th>user</th>\n",
       "      <th>place</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>344</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>300</td>\n",
       "      <td>425</td>\n",
       "      <td>2</td>\n",
       "      <td>299</td>\n",
       "      <td>324</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>299</td>\n",
       "      <td>132</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>300</td>\n",
       "      <td>311</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>348</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>300</td>\n",
       "      <td>279</td>\n",
       "      <td>4</td>\n",
       "      <td>299</td>\n",
       "      <td>290</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>300</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>299</td>\n",
       "      <td>243</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User_Id  Place_Id  Place_Ratings  user  place  rating\n",
       "0           1       179              3     0      0     3.0\n",
       "1           1       344              2     0      1     2.0\n",
       "2           1         5              5     0      2     5.0\n",
       "3           1       373              3     0      3     3.0\n",
       "4           1       101              4     0      4     4.0\n",
       "...       ...       ...            ...   ...    ...     ...\n",
       "9995      300       425              2   299    324     2.0\n",
       "9996      300        64              4   299    132     4.0\n",
       "9997      300       311              3   299    348     3.0\n",
       "9998      300       279              4   299    290     4.0\n",
       "9999      300       163              2   299    243     2.0\n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collfil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = collfil[['user', 'place']].values\n",
    "\n",
    "y = collfil['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(tf.keras.Model):\n",
    " \n",
    "  # Insialisasi fungsi\n",
    "  def __init__(self, num_users, num_places, embedding_size, **kwargs):\n",
    "    super(RecommenderNet, self).__init__(**kwargs)\n",
    "    self.num_users = num_users\n",
    "    self.num_places = num_places\n",
    "    self.embedding_size = embedding_size\n",
    "    self.user_embedding = layers.Embedding( # layer embedding user\n",
    "        num_users,\n",
    "        embedding_size,\n",
    "        embeddings_initializer = 'he_normal',\n",
    "        embeddings_regularizer = keras.regularizers.l2(1e-6)\n",
    "    )\n",
    "    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias\n",
    "    self.places_embedding = layers.Embedding( # layer embeddings places\n",
    "        num_places,\n",
    "        embedding_size,\n",
    "        embeddings_initializer = 'he_normal',\n",
    "        embeddings_regularizer = keras.regularizers.l2(1e-6)\n",
    "    )\n",
    "    self.places_bias = layers.Embedding(num_places, 1) # layer embedding places bias\n",
    " \n",
    "  def call(self, inputs):\n",
    "    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1\n",
    "    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2\n",
    "    places_vector = self.places_embedding(inputs[:, 1]) # memanggil layer embedding 3\n",
    "    places_bias = self.places_bias(inputs[:, 1]) # memanggil layer embedding 4\n",
    " \n",
    "    dot_user_places = tf.tensordot(user_vector, places_vector, 2) \n",
    " \n",
    "    x = dot_user_places + user_bias + places_bias\n",
    "    \n",
    "    return tf.nn.sigmoid(x) # activation sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecommenderNet(num_users, num_places, 50) # inisialisasi model\n",
    " \n",
    "# model compile\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0004),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Checking val_root_mean_squared_error at end of epoch...\")\n",
    "        if logs['val_root_mean_squared_error'] <= 0.25:\n",
    "               self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/210 [======================>.......] - ETA: 0s - loss: 0.7003 - root_mean_squared_error: 0.3489Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 1s 2ms/step - loss: 0.7004 - root_mean_squared_error: 0.3494 - val_loss: 0.7017 - val_root_mean_squared_error: 0.3531\n",
      "Epoch 2/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6964 - root_mean_squared_error: 0.3470Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6966 - root_mean_squared_error: 0.3467 - val_loss: 0.7011 - val_root_mean_squared_error: 0.3527\n",
      "Epoch 3/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6944 - root_mean_squared_error: 0.3454Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6946 - root_mean_squared_error: 0.3453 - val_loss: 0.7012 - val_root_mean_squared_error: 0.3527\n",
      "Epoch 4/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6940 - root_mean_squared_error: 0.3445Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6946 - root_mean_squared_error: 0.3454 - val_loss: 0.7011 - val_root_mean_squared_error: 0.3527\n",
      "Epoch 5/100\n",
      "185/210 [=========================>....] - ETA: 0s - loss: 0.6916 - root_mean_squared_error: 0.3437Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6922 - root_mean_squared_error: 0.3436 - val_loss: 0.7009 - val_root_mean_squared_error: 0.3525\n",
      "Epoch 6/100\n",
      "181/210 [========================>.....] - ETA: 0s - loss: 0.6932 - root_mean_squared_error: 0.3438Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6924 - root_mean_squared_error: 0.3438 - val_loss: 0.7009 - val_root_mean_squared_error: 0.3525\n",
      "Epoch 7/100\n",
      "168/210 [=======================>......] - ETA: 0s - loss: 0.6894 - root_mean_squared_error: 0.3436Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6893 - root_mean_squared_error: 0.3415 - val_loss: 0.7008 - val_root_mean_squared_error: 0.3524\n",
      "Epoch 8/100\n",
      "163/210 [======================>.......] - ETA: 0s - loss: 0.6884 - root_mean_squared_error: 0.3426Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6889 - root_mean_squared_error: 0.3412 - val_loss: 0.7006 - val_root_mean_squared_error: 0.3523\n",
      "Epoch 9/100\n",
      "166/210 [======================>.......] - ETA: 0s - loss: 0.6873 - root_mean_squared_error: 0.3393Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6874 - root_mean_squared_error: 0.3401 - val_loss: 0.7005 - val_root_mean_squared_error: 0.3523\n",
      "Epoch 10/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6880 - root_mean_squared_error: 0.3412Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6880 - root_mean_squared_error: 0.3406 - val_loss: 0.7002 - val_root_mean_squared_error: 0.3521\n",
      "Epoch 11/100\n",
      "198/210 [===========================>..] - ETA: 0s - loss: 0.6863 - root_mean_squared_error: 0.3404Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6862 - root_mean_squared_error: 0.3393 - val_loss: 0.6999 - val_root_mean_squared_error: 0.3518\n",
      "Epoch 12/100\n",
      "196/210 [===========================>..] - ETA: 0s - loss: 0.6843 - root_mean_squared_error: 0.3382Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6841 - root_mean_squared_error: 0.3378 - val_loss: 0.7001 - val_root_mean_squared_error: 0.3520\n",
      "Epoch 13/100\n",
      "169/210 [=======================>......] - ETA: 0s - loss: 0.6841 - root_mean_squared_error: 0.3376Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6847 - root_mean_squared_error: 0.3382 - val_loss: 0.6996 - val_root_mean_squared_error: 0.3516\n",
      "Epoch 14/100\n",
      "157/210 [=====================>........] - ETA: 0s - loss: 0.6831 - root_mean_squared_error: 0.3350Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6835 - root_mean_squared_error: 0.3373 - val_loss: 0.6999 - val_root_mean_squared_error: 0.3518\n",
      "Epoch 15/100\n",
      "160/210 [=====================>........] - ETA: 0s - loss: 0.6837 - root_mean_squared_error: 0.3362Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6827 - root_mean_squared_error: 0.3367 - val_loss: 0.6997 - val_root_mean_squared_error: 0.3517\n",
      "Epoch 16/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6793 - root_mean_squared_error: 0.3358Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6798 - root_mean_squared_error: 0.3346 - val_loss: 0.6997 - val_root_mean_squared_error: 0.3517\n",
      "Epoch 17/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6801 - root_mean_squared_error: 0.3346Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6801 - root_mean_squared_error: 0.3348 - val_loss: 0.6999 - val_root_mean_squared_error: 0.3518\n",
      "Epoch 18/100\n",
      "197/210 [===========================>..] - ETA: 0s - loss: 0.6802 - root_mean_squared_error: 0.3346Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6794 - root_mean_squared_error: 0.3343 - val_loss: 0.7000 - val_root_mean_squared_error: 0.3519\n",
      "Epoch 19/100\n",
      "158/210 [=====================>........] - ETA: 0s - loss: 0.6789 - root_mean_squared_error: 0.3334Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6790 - root_mean_squared_error: 0.3341 - val_loss: 0.7006 - val_root_mean_squared_error: 0.3523\n",
      "Epoch 20/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6769 - root_mean_squared_error: 0.3334Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6775 - root_mean_squared_error: 0.3329 - val_loss: 0.7006 - val_root_mean_squared_error: 0.3523\n",
      "Epoch 21/100\n",
      "166/210 [======================>.......] - ETA: 0s - loss: 0.6783 - root_mean_squared_error: 0.3340Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6784 - root_mean_squared_error: 0.3336 - val_loss: 0.7005 - val_root_mean_squared_error: 0.3522\n",
      "Epoch 22/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6753 - root_mean_squared_error: 0.3321Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6755 - root_mean_squared_error: 0.3315 - val_loss: 0.7010 - val_root_mean_squared_error: 0.3525\n",
      "Epoch 23/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6776 - root_mean_squared_error: 0.3330Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6773 - root_mean_squared_error: 0.3328 - val_loss: 0.7009 - val_root_mean_squared_error: 0.3525\n",
      "Epoch 24/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6757 - root_mean_squared_error: 0.3310Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6756 - root_mean_squared_error: 0.3315 - val_loss: 0.7011 - val_root_mean_squared_error: 0.3526\n",
      "Epoch 25/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6759 - root_mean_squared_error: 0.3307Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6752 - root_mean_squared_error: 0.3313 - val_loss: 0.7014 - val_root_mean_squared_error: 0.3528\n",
      "Epoch 26/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6753 - root_mean_squared_error: 0.3326Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6754 - root_mean_squared_error: 0.3314 - val_loss: 0.7018 - val_root_mean_squared_error: 0.3531\n",
      "Epoch 27/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6742 - root_mean_squared_error: 0.3297Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6746 - root_mean_squared_error: 0.3308 - val_loss: 0.7018 - val_root_mean_squared_error: 0.3531\n",
      "Epoch 28/100\n",
      "175/210 [========================>.....] - ETA: 0s - loss: 0.6744 - root_mean_squared_error: 0.3316Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6732 - root_mean_squared_error: 0.3297 - val_loss: 0.7018 - val_root_mean_squared_error: 0.3531\n",
      "Epoch 29/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6729 - root_mean_squared_error: 0.3309Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6744 - root_mean_squared_error: 0.3307 - val_loss: 0.7017 - val_root_mean_squared_error: 0.3530\n",
      "Epoch 30/100\n",
      "197/210 [===========================>..] - ETA: 0s - loss: 0.6711 - root_mean_squared_error: 0.3285Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6714 - root_mean_squared_error: 0.3285 - val_loss: 0.7020 - val_root_mean_squared_error: 0.3532\n",
      "Epoch 31/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6700 - root_mean_squared_error: 0.3279Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6703 - root_mean_squared_error: 0.3276 - val_loss: 0.7022 - val_root_mean_squared_error: 0.3533\n",
      "Epoch 32/100\n",
      "191/210 [==========================>...] - ETA: 0s - loss: 0.6719 - root_mean_squared_error: 0.3298Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6721 - root_mean_squared_error: 0.3290 - val_loss: 0.7022 - val_root_mean_squared_error: 0.3533\n",
      "Epoch 33/100\n",
      "198/210 [===========================>..] - ETA: 0s - loss: 0.6705 - root_mean_squared_error: 0.3278Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6702 - root_mean_squared_error: 0.3276 - val_loss: 0.7022 - val_root_mean_squared_error: 0.3533\n",
      "Epoch 34/100\n",
      "164/210 [======================>.......] - ETA: 0s - loss: 0.6709 - root_mean_squared_error: 0.3275Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6704 - root_mean_squared_error: 0.3278 - val_loss: 0.7025 - val_root_mean_squared_error: 0.3536\n",
      "Epoch 35/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6691 - root_mean_squared_error: 0.3254Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6691 - root_mean_squared_error: 0.3268 - val_loss: 0.7028 - val_root_mean_squared_error: 0.3538\n",
      "Epoch 36/100\n",
      "201/210 [===========================>..] - ETA: 0s - loss: 0.6694 - root_mean_squared_error: 0.3276Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6698 - root_mean_squared_error: 0.3273 - val_loss: 0.7028 - val_root_mean_squared_error: 0.3537\n",
      "Epoch 37/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6692 - root_mean_squared_error: 0.3267Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6693 - root_mean_squared_error: 0.3270 - val_loss: 0.7030 - val_root_mean_squared_error: 0.3539\n",
      "Epoch 38/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6678 - root_mean_squared_error: 0.3263Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6681 - root_mean_squared_error: 0.3260 - val_loss: 0.7031 - val_root_mean_squared_error: 0.3540\n",
      "Epoch 39/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6684 - root_mean_squared_error: 0.3283Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6685 - root_mean_squared_error: 0.3264 - val_loss: 0.7032 - val_root_mean_squared_error: 0.3540\n",
      "Epoch 40/100\n",
      "201/210 [===========================>..] - ETA: 0s - loss: 0.6689 - root_mean_squared_error: 0.3262Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6688 - root_mean_squared_error: 0.3266 - val_loss: 0.7030 - val_root_mean_squared_error: 0.3538\n",
      "Epoch 41/100\n",
      "191/210 [==========================>...] - ETA: 0s - loss: 0.6679 - root_mean_squared_error: 0.3259Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6683 - root_mean_squared_error: 0.3262 - val_loss: 0.7031 - val_root_mean_squared_error: 0.3539\n",
      "Epoch 42/100\n",
      "162/210 [======================>.......] - ETA: 0s - loss: 0.6671 - root_mean_squared_error: 0.3256Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6678 - root_mean_squared_error: 0.3258 - val_loss: 0.7035 - val_root_mean_squared_error: 0.3542\n",
      "Epoch 43/100\n",
      "188/210 [=========================>....] - ETA: 0s - loss: 0.6667 - root_mean_squared_error: 0.3259Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6672 - root_mean_squared_error: 0.3254 - val_loss: 0.7040 - val_root_mean_squared_error: 0.3546\n",
      "Epoch 44/100\n",
      "167/210 [======================>.......] - ETA: 0s - loss: 0.6679 - root_mean_squared_error: 0.3248Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6671 - root_mean_squared_error: 0.3254 - val_loss: 0.7039 - val_root_mean_squared_error: 0.3545\n",
      "Epoch 45/100\n",
      "166/210 [======================>.......] - ETA: 0s - loss: 0.6663 - root_mean_squared_error: 0.3251Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6666 - root_mean_squared_error: 0.3249 - val_loss: 0.7040 - val_root_mean_squared_error: 0.3545\n",
      "Epoch 46/100\n",
      "204/210 [============================>.] - ETA: 0s - loss: 0.6672 - root_mean_squared_error: 0.3255Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6673 - root_mean_squared_error: 0.3254 - val_loss: 0.7042 - val_root_mean_squared_error: 0.3546\n",
      "Epoch 47/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6664 - root_mean_squared_error: 0.3244Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6660 - root_mean_squared_error: 0.3245 - val_loss: 0.7044 - val_root_mean_squared_error: 0.3548\n",
      "Epoch 48/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6649 - root_mean_squared_error: 0.3229Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6653 - root_mean_squared_error: 0.3240 - val_loss: 0.7046 - val_root_mean_squared_error: 0.3550\n",
      "Epoch 49/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6652 - root_mean_squared_error: 0.3230Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6642 - root_mean_squared_error: 0.3232 - val_loss: 0.7048 - val_root_mean_squared_error: 0.3551\n",
      "Epoch 50/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6662 - root_mean_squared_error: 0.3245Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6649 - root_mean_squared_error: 0.3237 - val_loss: 0.7051 - val_root_mean_squared_error: 0.3553\n",
      "Epoch 51/100\n",
      "205/210 [============================>.] - ETA: 0s - loss: 0.6649 - root_mean_squared_error: 0.3239Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6650 - root_mean_squared_error: 0.3238 - val_loss: 0.7051 - val_root_mean_squared_error: 0.3552\n",
      "Epoch 52/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6639 - root_mean_squared_error: 0.3230Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6643 - root_mean_squared_error: 0.3232 - val_loss: 0.7055 - val_root_mean_squared_error: 0.3555\n",
      "Epoch 53/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6645 - root_mean_squared_error: 0.3257Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6644 - root_mean_squared_error: 0.3233 - val_loss: 0.7062 - val_root_mean_squared_error: 0.3559\n",
      "Epoch 54/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6648 - root_mean_squared_error: 0.3220Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6644 - root_mean_squared_error: 0.3233 - val_loss: 0.7062 - val_root_mean_squared_error: 0.3560\n",
      "Epoch 55/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6646 - root_mean_squared_error: 0.3231Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6644 - root_mean_squared_error: 0.3233 - val_loss: 0.7066 - val_root_mean_squared_error: 0.3562\n",
      "Epoch 56/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6636 - root_mean_squared_error: 0.3231Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6640 - root_mean_squared_error: 0.3231 - val_loss: 0.7063 - val_root_mean_squared_error: 0.3560\n",
      "Epoch 57/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6644 - root_mean_squared_error: 0.3239Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6639 - root_mean_squared_error: 0.3230 - val_loss: 0.7064 - val_root_mean_squared_error: 0.3561\n",
      "Epoch 58/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6629 - root_mean_squared_error: 0.3215Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6621 - root_mean_squared_error: 0.3217 - val_loss: 0.7064 - val_root_mean_squared_error: 0.3561\n",
      "Epoch 59/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6622 - root_mean_squared_error: 0.3213Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6622 - root_mean_squared_error: 0.3218 - val_loss: 0.7065 - val_root_mean_squared_error: 0.3562\n",
      "Epoch 60/100\n",
      "189/210 [==========================>...] - ETA: 0s - loss: 0.6630 - root_mean_squared_error: 0.3229Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6628 - root_mean_squared_error: 0.3222 - val_loss: 0.7065 - val_root_mean_squared_error: 0.3561\n",
      "Epoch 61/100\n",
      "181/210 [========================>.....] - ETA: 0s - loss: 0.6609 - root_mean_squared_error: 0.3207Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6619 - root_mean_squared_error: 0.3215 - val_loss: 0.7068 - val_root_mean_squared_error: 0.3563\n",
      "Epoch 62/100\n",
      "182/210 [=========================>....] - ETA: 0s - loss: 0.6630 - root_mean_squared_error: 0.3210Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6622 - root_mean_squared_error: 0.3218 - val_loss: 0.7067 - val_root_mean_squared_error: 0.3563\n",
      "Epoch 63/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6645 - root_mean_squared_error: 0.3228Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6638 - root_mean_squared_error: 0.3229 - val_loss: 0.7070 - val_root_mean_squared_error: 0.3564\n",
      "Epoch 64/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6624 - root_mean_squared_error: 0.3224Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6631 - root_mean_squared_error: 0.3224 - val_loss: 0.7072 - val_root_mean_squared_error: 0.3566\n",
      "Epoch 65/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6622 - root_mean_squared_error: 0.3222Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6623 - root_mean_squared_error: 0.3217 - val_loss: 0.7073 - val_root_mean_squared_error: 0.3566\n",
      "Epoch 66/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6626 - root_mean_squared_error: 0.3218Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6614 - root_mean_squared_error: 0.3212 - val_loss: 0.7073 - val_root_mean_squared_error: 0.3566\n",
      "Epoch 67/100\n",
      "165/210 [======================>.......] - ETA: 0s - loss: 0.6600 - root_mean_squared_error: 0.3201Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6603 - root_mean_squared_error: 0.3203 - val_loss: 0.7076 - val_root_mean_squared_error: 0.3568\n",
      "Epoch 68/100\n",
      "161/210 [======================>.......] - ETA: 0s - loss: 0.6626 - root_mean_squared_error: 0.3219Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6619 - root_mean_squared_error: 0.3215 - val_loss: 0.7078 - val_root_mean_squared_error: 0.3569\n",
      "Epoch 69/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6610 - root_mean_squared_error: 0.3212Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6622 - root_mean_squared_error: 0.3217 - val_loss: 0.7082 - val_root_mean_squared_error: 0.3572\n",
      "Epoch 70/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6632 - root_mean_squared_error: 0.3225Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6623 - root_mean_squared_error: 0.3218 - val_loss: 0.7083 - val_root_mean_squared_error: 0.3572\n",
      "Epoch 71/100\n",
      "169/210 [=======================>......] - ETA: 0s - loss: 0.6633 - root_mean_squared_error: 0.3230Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6629 - root_mean_squared_error: 0.3222 - val_loss: 0.7083 - val_root_mean_squared_error: 0.3572\n",
      "Epoch 72/100\n",
      "161/210 [======================>.......] - ETA: 0s - loss: 0.6601 - root_mean_squared_error: 0.3206Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6614 - root_mean_squared_error: 0.3211 - val_loss: 0.7084 - val_root_mean_squared_error: 0.3573\n",
      "Epoch 73/100\n",
      "205/210 [============================>.] - ETA: 0s - loss: 0.6618 - root_mean_squared_error: 0.3215Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6615 - root_mean_squared_error: 0.3213 - val_loss: 0.7083 - val_root_mean_squared_error: 0.3572\n",
      "Epoch 74/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6611 - root_mean_squared_error: 0.3216Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6612 - root_mean_squared_error: 0.3211 - val_loss: 0.7083 - val_root_mean_squared_error: 0.3572\n",
      "Epoch 75/100\n",
      "177/210 [========================>.....] - ETA: 0s - loss: 0.6605 - root_mean_squared_error: 0.3213Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6612 - root_mean_squared_error: 0.3210 - val_loss: 0.7085 - val_root_mean_squared_error: 0.3574\n",
      "Epoch 76/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6597 - root_mean_squared_error: 0.3206Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6598 - root_mean_squared_error: 0.3200 - val_loss: 0.7090 - val_root_mean_squared_error: 0.3576\n",
      "Epoch 77/100\n",
      "169/210 [=======================>......] - ETA: 0s - loss: 0.6604 - root_mean_squared_error: 0.3220Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6617 - root_mean_squared_error: 0.3214 - val_loss: 0.7092 - val_root_mean_squared_error: 0.3578\n",
      "Epoch 78/100\n",
      "190/210 [==========================>...] - ETA: 0s - loss: 0.6603 - root_mean_squared_error: 0.3207Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.6602 - root_mean_squared_error: 0.3203 - val_loss: 0.7093 - val_root_mean_squared_error: 0.3578\n",
      "Epoch 79/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6617 - root_mean_squared_error: 0.3217Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6617 - root_mean_squared_error: 0.3214 - val_loss: 0.7095 - val_root_mean_squared_error: 0.3579\n",
      "Epoch 80/100\n",
      "170/210 [=======================>......] - ETA: 0s - loss: 0.6613 - root_mean_squared_error: 0.3213Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6613 - root_mean_squared_error: 0.3211 - val_loss: 0.7096 - val_root_mean_squared_error: 0.3580\n",
      "Epoch 81/100\n",
      "163/210 [======================>.......] - ETA: 0s - loss: 0.6625 - root_mean_squared_error: 0.3219Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6618 - root_mean_squared_error: 0.3215 - val_loss: 0.7098 - val_root_mean_squared_error: 0.3581\n",
      "Epoch 82/100\n",
      "164/210 [======================>.......] - ETA: 0s - loss: 0.6585 - root_mean_squared_error: 0.3185Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6590 - root_mean_squared_error: 0.3194 - val_loss: 0.7101 - val_root_mean_squared_error: 0.3583\n",
      "Epoch 83/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6598 - root_mean_squared_error: 0.3197Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6596 - root_mean_squared_error: 0.3197 - val_loss: 0.7105 - val_root_mean_squared_error: 0.3586\n",
      "Epoch 84/100\n",
      "203/210 [============================>.] - ETA: 0s - loss: 0.6581 - root_mean_squared_error: 0.3183Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6587 - root_mean_squared_error: 0.3191 - val_loss: 0.7107 - val_root_mean_squared_error: 0.3587\n",
      "Epoch 85/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6601 - root_mean_squared_error: 0.3208Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6611 - root_mean_squared_error: 0.3209 - val_loss: 0.7109 - val_root_mean_squared_error: 0.3588\n",
      "Epoch 86/100\n",
      "168/210 [=======================>......] - ETA: 0s - loss: 0.6597 - root_mean_squared_error: 0.3198Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6601 - root_mean_squared_error: 0.3202 - val_loss: 0.7115 - val_root_mean_squared_error: 0.3592\n",
      "Epoch 87/100\n",
      "176/210 [========================>.....] - ETA: 0s - loss: 0.6625 - root_mean_squared_error: 0.3208Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6605 - root_mean_squared_error: 0.3205 - val_loss: 0.7112 - val_root_mean_squared_error: 0.3590\n",
      "Epoch 88/100\n",
      "176/210 [========================>.....] - ETA: 0s - loss: 0.6589 - root_mean_squared_error: 0.3193Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6602 - root_mean_squared_error: 0.3203 - val_loss: 0.7113 - val_root_mean_squared_error: 0.3591\n",
      "Epoch 89/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6590 - root_mean_squared_error: 0.3191Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6596 - root_mean_squared_error: 0.3199 - val_loss: 0.7116 - val_root_mean_squared_error: 0.3593\n",
      "Epoch 90/100\n",
      "173/210 [=======================>......] - ETA: 0s - loss: 0.6592 - root_mean_squared_error: 0.3199Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6587 - root_mean_squared_error: 0.3192 - val_loss: 0.7119 - val_root_mean_squared_error: 0.3594\n",
      "Epoch 91/100\n",
      "172/210 [=======================>......] - ETA: 0s - loss: 0.6617 - root_mean_squared_error: 0.3220Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6614 - root_mean_squared_error: 0.3212 - val_loss: 0.7119 - val_root_mean_squared_error: 0.3595\n",
      "Epoch 92/100\n",
      "176/210 [========================>.....] - ETA: 0s - loss: 0.6581 - root_mean_squared_error: 0.3187Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6584 - root_mean_squared_error: 0.3189 - val_loss: 0.7123 - val_root_mean_squared_error: 0.3597\n",
      "Epoch 93/100\n",
      "178/210 [========================>.....] - ETA: 0s - loss: 0.6612 - root_mean_squared_error: 0.3203Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6600 - root_mean_squared_error: 0.3202 - val_loss: 0.7123 - val_root_mean_squared_error: 0.3597\n",
      "Epoch 94/100\n",
      "197/210 [===========================>..] - ETA: 0s - loss: 0.6598 - root_mean_squared_error: 0.3205Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6600 - root_mean_squared_error: 0.3201 - val_loss: 0.7125 - val_root_mean_squared_error: 0.3598\n",
      "Epoch 95/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6588 - root_mean_squared_error: 0.3193Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6588 - root_mean_squared_error: 0.3192 - val_loss: 0.7128 - val_root_mean_squared_error: 0.3600\n",
      "Epoch 96/100\n",
      "176/210 [========================>.....] - ETA: 0s - loss: 0.6591 - root_mean_squared_error: 0.3206Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6594 - root_mean_squared_error: 0.3197 - val_loss: 0.7126 - val_root_mean_squared_error: 0.3599\n",
      "Epoch 97/100\n",
      "176/210 [========================>.....] - ETA: 0s - loss: 0.6610 - root_mean_squared_error: 0.3201Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6602 - root_mean_squared_error: 0.3203 - val_loss: 0.7128 - val_root_mean_squared_error: 0.3600\n",
      "Epoch 98/100\n",
      "169/210 [=======================>......] - ETA: 0s - loss: 0.6600 - root_mean_squared_error: 0.3205Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6605 - root_mean_squared_error: 0.3205 - val_loss: 0.7130 - val_root_mean_squared_error: 0.3601\n",
      "Epoch 99/100\n",
      "174/210 [=======================>......] - ETA: 0s - loss: 0.6593 - root_mean_squared_error: 0.3197Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6597 - root_mean_squared_error: 0.3199 - val_loss: 0.7133 - val_root_mean_squared_error: 0.3603\n",
      "Epoch 100/100\n",
      "171/210 [=======================>......] - ETA: 0s - loss: 0.6565 - root_mean_squared_error: 0.3183Checking val_root_mean_squared_error at end of epoch...\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 0.6578 - root_mean_squared_error: 0.3185 - val_loss: 0.7132 - val_root_mean_squared_error: 0.3603\n"
     ]
    }
   ],
   "source": [
    "# Memulai training\n",
    "history = model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    epochs = 100,\n",
    "    validation_data = (x_test, y_test),\n",
    "    callbacks = [myCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.title('model_metrics')\n",
    "plt.ylabel('root_mean_squared_error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_Id          int64\n",
      "Place_Id         int64\n",
      "Place_Ratings    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "place_df = update[['Place_Id','Place_Name','new_category','Rating','Price']]\n",
    "place_df.columns = ['id','place_name','category','rating','price']\n",
    "df = rat.copy()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = df.User_Id.sample(1).iloc[0]\n",
    "place_visited_by_user = df[df.User_Id == user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat data lokasi yang belum dikunjungi user\n",
    "place_not_rated = place_df[~place_df['id'].isin(\n",
    "    place_visited_by_user.Place_Id.values)]['id']\n",
    "place_not_rated = list(\n",
    "    set(place_not_rated).intersection(set(place_to_place_encoded.keys()))\n",
    ")\n",
    "\n",
    "place_not_rated = [\n",
    "    [place_to_place_encoded.get(x)] for x in place_not_rated]\n",
    "user_encoder = user_to_user_encoded.get(user_id)\n",
    "user_place_array = np.hstack(\n",
    "    ([[user_encoder]] * len(place_not_rated), place_not_rated)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 653us/step\n",
      "Skyrink - Mall Taman Anggrek : ['Hiburan', 'Olahraga']\n",
      "Alive Museum Ancol : ['Hiburan', 'Keluarga']\n",
      "Masjid Agung Trans Studio Bandung : ['Budaya']\n",
      "Taman Pandanaran : ['Budaya']\n",
      "Taman Flora Bratang Surabaya : ['Alam', 'Relaksasi']\n"
     ]
    }
   ],
   "source": [
    "ratings = model.predict(user_place_array).flatten()\n",
    "top_ratings_indices = ratings.argsort()[-5:][::-1]\n",
    "recommended_place_ids = [\n",
    "    place_encoded_to_place.get(place_not_rated[x][0]) for x in top_ratings_indices\n",
    "]\n",
    " \n",
    "# print('Daftar rekomendasi untuk: {}'.format('User ' + str(user_id)))\n",
    "# print('===' * 15,'\\n')\n",
    "# print('----' * 15)\n",
    "# print('Tempat dengan rating wisata paling tinggi dari user')\n",
    "# print('----' * 15)\n",
    " \n",
    "top_place_user = (\n",
    "    place_visited_by_user.sort_values(\n",
    "        by = 'Place_Ratings',\n",
    "        ascending=False\n",
    "    )\n",
    "    .head(5)\n",
    "    .Place_Id.values\n",
    ")\n",
    " \n",
    "place_df_rows = place_df[place_df['id'].isin(top_place_user)]\n",
    "for row in place_df_rows.itertuples():\n",
    "    print(row.place_name, ':', row.category)\n",
    "\n",
    "# print('')\n",
    "# print('----' * 15)\n",
    "# print('Top 5 place recommendation')\n",
    "# print('----' * 15)\n",
    " \n",
    "cf_recommendation = place_df[place_df['id'].isin(recommended_place_ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>place_name</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Monumen Yogya Kembali</td>\n",
       "      <td>['Budaya']</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>Puncak Gunung Api Purba - Nglanggeran</td>\n",
       "      <td>['Alam', 'Petualangan']</td>\n",
       "      <td>4.7</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>183</td>\n",
       "      <td>Jogja Bay Pirates Adventure Waterpark</td>\n",
       "      <td>['Hiburan', 'Keluarga']</td>\n",
       "      <td>4.4</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>Pantai Ngrenehan</td>\n",
       "      <td>['Alam', 'Romantis', 'Keluarga']</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>Sanghyang Heuleut</td>\n",
       "      <td>['Alam', 'Petualangan']</td>\n",
       "      <td>4.4</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                             place_name  \\\n",
       "96    97                  Monumen Yogya Kembali   \n",
       "138  139  Puncak Gunung Api Purba - Nglanggeran   \n",
       "182  183  Jogja Bay Pirates Adventure Waterpark   \n",
       "195  196                       Pantai Ngrenehan   \n",
       "299  300                      Sanghyang Heuleut   \n",
       "\n",
       "                             category  rating   price  \n",
       "96                         ['Budaya']     4.5   15000  \n",
       "138           ['Alam', 'Petualangan']     4.7   10000  \n",
       "182           ['Hiburan', 'Keluarga']     4.4  150000  \n",
       "195  ['Alam', 'Romantis', 'Keluarga']     4.4    3000  \n",
       "299           ['Alam', 'Petualangan']     4.4   10000  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29140\\2547154286.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cf_recommendation.drop([\"place_name\", \"category\", \"rating\", \"price\"], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cf_recommendation.drop([\"place_name\", \"category\", \"rating\", \"price\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29140\\1174431504.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cf_recommendation.rename(columns={'id': 'Place_Id'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cf_recommendation.rename(columns={'id': 'Place_Id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_scoring(df):\n",
    "    # this function will return score for each user recommended items\n",
    "    df['score'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_29140\\2018131887.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score'] = 1\n"
     ]
    }
   ],
   "source": [
    "cf_recommendation = give_scoring(cf_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Place_Id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Place_Id  score\n",
       "96         97      1\n",
       "138       139      1\n",
       "182       183      1\n",
       "195       196      1\n",
       "299       300      1"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_recommendation.to_csv('../data/output/cf_recommendation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
